# Hugging Face Service

This Python script utilizes the Hugging Face library to create a service that answers user queries using various machine learning models hosted on the Hugging Face Hub.

## Overview

The Hugging Face Service consists of a Python class called `HuggingFaceService`, which is designed to interact with Hugging Face models. It allows users to input queries and receive responses generated by the selected model.


## Python Environment Setup

1. Install anaconda on your deive.
2. Lisiting Environments
   ```bash
   conda info --envs
   ```
3. Creating and activating an environment:
   ```bash
   conda create -n my-first-env python=3.10

   source activate my-first-env
   ```
4. After the environment is created, install a python library name langchain the newly creaated environment:
   ```bash
   pip install langchain
   ```
   
## Installation

1. Clone this repository to your local machine.
2. Install the required dependencies using pip:

```bash
pip install -r requirements.txt
```
Usage

Obtain your Hugging Face API token. If you don't have one, you can sign up and get your token here(https://huggingface.co/docs/api-inference/en/index).



# Hugging Face Q&A Service

This repository contains scripts for utilizing different Hugging Face models to perform question-answering tasks. Three different approaches are demonstrated:

1. **Googleflan1.py**: Utilizes the `langchain` library to create a service for answering user queries using various Hugging Face models.

2. **Googleflan2.py**: Implements a question-answering system using the `transformers` library and a specific Hugging Face model (`google/flan-t5-xxl`).

3. **Googleflan3.py**: Uses the `transformers` library to create a question-answering pipeline with the Flan T5 XXL model.

## File Descriptions

- **File 1 (googleflan1.py.py)**: Contains a class `HuggingFaceService` that interacts with Hugging Face models using the `langchain` library. Users can input queries, and the service generates responses using the specified Hugging Face model.

- **File 2 (googleflan2.py.py)**: Implements a question-answering system using the `transformers` library. It loads the specified Hugging Face model (`google/flan-t5-xxl`), prompts the user to input a question, and provides the answer based on the model's predictions.

- **File 3 (googleflan3.py)**: Demonstrates the use of the `transformers` library to create a question-answering pipeline with the Flan T5 XXL model. It prompts the user to input a question and provides the answer using the pipeline.

## Usage

1. **FGoogleflan1**: Run `file1.py` and follow the prompts to input your Hugging Face API token and queries.

2. **Googleflan2**: Run `file2.py` and input a question when prompted. The script will output the answer based on the specified Hugging Face model.

3. **Googleflan3**: Run `file3.py` and input a question when prompted. The script will output the answer using the Flan T5 XXL model via the question-answering pipeline.





